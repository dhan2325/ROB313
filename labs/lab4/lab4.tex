\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 4}
\author{Daniel Han: 1006842534}
\date{March 2023}

\begin{document}

\maketitle


\section{Xavier Initialization}
\subsection{Finding variance of weights}
Xavier initialization determines the weights for a layer in a neural network for the first iteration of a weight optimization by sampling a Gaussian distribution. Each element in the weight matrix (elements denoted $w_{ij}$) is independently and identically distributed. The Gaussian ditribution from which the weights are selected is assigned a mean of zero, and the variance of the weights is selected such that the variance of the outputs of the layer are equal to the variance of its inputs.

In this question, with the assumption that each input $x_j$ is sampled from a normal distribution $\mathcal{N}(0, \eta^2)$. Additionally, it was specified that the activation was the identity (i.e. $\sigma (x) = x$). From the information provided, and analytical expression for the variance of the outputs $z_i$ could be found as a probabilistic expectation. The varaince of the random variable $z_i$ can be written as shown below:

\begin{equation}
Var(z_i) = E[z_{i}^{2}- E[z_i]^2]
\end{equation}

Since the distribution of $z_i$ is defined as having zero mean, we can omit the $E[z_i]$ term in the epxression. Substituting the expression for $z_i$ provided,

\begin{align}
Var(z_i) &= E[z_i] \\
Var(z_i) &= E\Big[(\sum_{j=1}^{D} x_j w_{ij})^2\Big]\\
Var(z_i) &= E\Big[\sum_{j=1}^{D} \sum_{k=1}^{D} x_j x_k w_{ij} w_{ik}\Big]\\
Var(z_i) &= \sum_{j=1}^{D} \sum_{k=1}^{D} E \Big[x_j x_k w_{ij} w_{ik}\Big]
\end{align}

It should be noted that in (5), all four terms in the product $x_j x_k w_{ij} w_{ik}$ are independently sampled. As such, in terms where $j \neq k$, the expectation of the product is the product of the expectation, and since all four terms are sampled from a zero-mean distribution, these terms go to zero. For terms where $j=k$, the weights and inputs are still independent, so the following expression for variance can be obtained:

\begin{align}
Var(z_i) &= \sum_{j=1}^{D} \Big[x_{j}^2 w_{ij}^2\Big] \\
Var(z_i) & = \sum_{j=1}^{N} E\Big[x_{j}^2\Big] E\Big[w_{ij}^2\Big] \\
Var(z_i) &= \sum_{j=1}^{N} \eta^2 \varepsilon^2 \\
\eta^2 &= D\eta^2 \varepsilon^2 \\
\varepsilon &= \frac{1}{\sqrt{D}}
\end{align}

Thus, for the Xavier Initialization of the weights, the variance for the normal distribution for the weight matrix is equal to the reciprocal of the number of inputs to the hidden layer.

\subsection{Python Implementation of init\_xavier}
The function \verb+init\_xavier+ is passed as inputs the dimensions of the weight matrix (m being the number of outputs, n the number of inputs), which is sufficient to generate a weight matrix. The \verb+numpy.random.randn+ function can be used to generate a matrix of arbitrary size, for which each element is sampled from the standard normal distribution. Multiplying each element by the desired standard deviation, the resulting matrix is equivalent to one that was generated by sampling each $w_{ij}$ from the $\mathcal{N}(0,\eta^2)$ distribution.


\section{`logsumexp' trick}
The `logsumexp' trick uses an algebraic identity to compute the logarithm of the sum of the exponential of a series of numbers in a numerically stable manner.

The naive implementation is less numerically stable and will be more largely impacted by truncation/rounding errors. A mathematical expression for the naive implementation can be writtern as follows:

\begin{equation}
o_i = o_i - \log(\sum e^{o_i})
\end{equation}
% I'm not sure I'm interpreting this correctly?

Where $o_i$ denotes the ith element of the output vector. This implementation involves taking the exponential of each number, which can lead to issues when numbers are very large or small. Large terms will dominate the sum and possibly lead to numeric overflow, while smaller terms could potentially lead to underflows and be lost in the noise of larger terms. However, using logarithm properties, the expression can be rewritten as:

\begin{equation}
o_i = o_i - (x' + \log(1 + \sum \exp(x_i - x')))
\end{equation} 

Where $x' = max(x_i)$. This new form is closer to the backend computations performed by \verb+logsumexp+, and is more numerically stable since it involves the exponentials, addition, and subtraction of smaller numbers, thereby rendering it more numerically stable.

Thus, though the two forms may seem equivalent from an algebraic standpoint, due to the limitations of the hardware involved in the computations, the `logsumexp' trick yields better, more numerically stable results.


\end{document}