\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 4}
\author{Daniel Han: 1006842534}
\date{March 2023}

\begin{document}

\maketitle


\section{Xavier Initialization}
\subsection{Finding variance of weights}
Xavier initialization determines the weights for a layer in a neural network for the first iteration of a weight optimization by sampling a Gaussian distribution. Each element in the weight matrix (elements denoted $w_{ij}$) is independently and identically distributed. The Gaussian ditribution from which the weights are selected is assigned a mean of zero, and the variance of the weights is selected such that the variance of the outputs of the layer are equal to the variance of its inputs.

In this question, with the assumption that each input $x_j$ is sampled from a normal distribution $\mathcal{N}(0, \eta^2)$. Additionally, it was specified that the activation was the identity (i.e. $\sigma (x) = x$). From the information provided, and analytical expression for the variance of the outputs $z_i$ could be found as a probabilistic expectation. The varaince of the random variable $z_i$ can be written as shown below:

\begin{equation}
Var(z_i) = E[z_{i}^{2}- E[z_i]^2]
\end{equation}

Since the distribution of $z_i$ is defined as having zero mean, we can omit the $E[z_i]$ term in the epxression. Substituting the expression for $z_i$ provided,

\begin{align}
Var(z_i) &= E[z_i] \\
Var(z_i) &= E\Big[(\sum_{j=1}^{D} x_j w_{ij})^2\Big]\\
Var(z_i) &= E\Big[\sum_{j=1}^{D} \sum_{k=1}^{D} x_j x_k w_{ij} w_{ik}\Big]\\
Var(z_i) &= \sum_{j=1}^{D} \sum_{k=1}^{D} E \Big[x_j x_k w_{ij} w_{ik}\Big]
\end{align}

It should be noted that in (5), all four terms in the product $x_j x_k w_{ij} w_{ik}$ are independently sampled. As such, in terms where $j \neq k$, the expectation of the product is the product of the expectation, and since all four terms are sampled from a zero-mean distribution, these terms go to zero. For terms where $j=k$, the weights and inputs are still independent, so the following expression for variance can be obtained:

\begin{align}
Var(z_i) &= \sum_{j=1}^{D} \Big[x_{j}^2 w_{ij}^2\Big] \\
Var(z_i) & = \sum_{j=1}^{N} E\Big[x_{j}^2\Big] E\Big[w_{ij}^2\Big] \\
Var(z_i) &= \sum_{j=1}^{N} \eta^2 \varepsilon^2 \\
\eta^2 &= D\eta^2 \varepsilon^2 \\
\varepsilon &= \frac{1}{\sqrt{D}}
\end{align}

Thus, for the Xavier Initialization of the weights, the variance for the normal distribution for the weight matrix is equal to the reciprocal of the number of inputs to the hidden layer.

\subsection{Python Implementation of init\_xavier}
The function \verb+init\_xavier+ is passed as inputs the dimensions of the weight matrix (m being the number of outputs, n the number of inputs), which is sufficient to generate a weight matrix. The \verb+numpy.random.randn+ function can be used to generate a matrix of arbitrary size, for which each element is sampled from the standard normal distribution. Multiplying each element by the desired standard deviation, the resulting matrix is equivalent to one that was generated by sampling each $w_{ij}$ from the $\mathcal{N}(0,\eta^2)$ distribution.

\newpage
\section{`logsumexp' trick}
The `logsumexp' trick uses an algebraic identity to compute the logarithm of the sum of the exponential of a series of numbers in a numerically stable manner.

The naive implementation is less numerically stable and will be more largely impacted by truncation/rounding errors. A mathematical expression for the naive implementation can be writtern as follows:

\begin{equation}
o_i = o_i - \log(\sum e^{o_i})
\end{equation}
% I'm not sure I'm interpreting this correctly?

Where $o_i$ denotes the ith element of the output vector. This implementation involves taking the exponential of each number, which can lead to issues when numbers are very large or small. Large terms will dominate the sum and possibly lead to numeric overflow, while smaller terms could potentially lead to underflows and be lost in the noise of larger terms. However, using logarithm properties, the expression can be rewritten as:

\begin{equation}
o_i = o_i - (x' + \log(1 + \sum \exp(x_i - x')))
\end{equation} 

Where $x' = max(x_i)$. This new form is closer to the backend computations performed by \verb+logsumexp+, and is more numerically stable since it involves the exponentials, addition, and subtraction of smaller numbers, thereby rendering it more numerically stable.

Thus, though the two forms may seem equivalent from an algebraic standpoint, due to the limitations of the hardware involved in the computations, the `logsumexp' trick yields better, more numerically stable results. The code for the function is shown below.

\begin{verbatim}
def init_xavier(m, n, rs=npr.RandomState(0)):
    return rs.randn(m, n) / np.sqrt(m)
\end{verbatim}

\newpage
\section{Mean Log Likelihood of the dataset}
The dataset likelihood is the product of all class-conditional probabilities of all inputs in the dataset, producing a nested product expression. Taking the logarithm of the expression, the product beceomes a sum, which is displayed below:

\begin{equation}
\prod_{i = 1}^N \prod_{j=1}^K  \hat{f}_{j} (\mathbf{x}^{(i)}, \mathbf{W}) ^{y_{j}^{i}} = \sum_{i = 1}^N \sum_{j=1}^K y_{j}^{i} \log(\hat{f}_{j} (\mathbf{x}^{(i)}, \mathbf{W}))
\end{equation}

Where $\hat{f}_{j} (\mathbf{x}^{(i)}, \mathbf{W})$ denotes the probability of the $i^{th}$ input's $j^{th}$ classification being true. To calculate this sum, the function neural\_net\_predict was used to compute the normalized class log-probabilities, then the element-wise product of the normalized class log-probabilies and the target outputs. Then, the mean of all elements in the resulting matrix was computed, then multiplied by the number of elements in each row, to get the average across the inputs.
\begin{verbatim}
def neural_net_predict(params, inputs):
    for W, b in params:
        outputs = np.dot(inputs, W) + b # general formula for transformation of a layer in neural net
        inputs = relu(outputs)
    return outputs - logsumexp(outputs, axis=1, keepdims=True)


def mean_log_like(params, inputs, targets):
    # raise NotImplementedError("mean_log_like function not completed (see Q3).")
    # print('computing mean-log-like')
    return np.mean(neural_net_predict(params, inputs) * targets) * 10
\end{verbatim}
\newpage
\section{Parameter Tuning for MNIST}
Selection of hyperparamters for the MNIST neural network was done iteratively in a qualitative fashion. Since the hyperparamters for optimization did not dictate the viability of the model but rather how it owuld be implemented numerically, the gradient descent hyperparameters were set to be very standard as various neural networks structures were tested for. First, Gradient Descent was set to be full-batch, the learning rate was made smaller to increase chances of numeric stability (at the expense of computation time) and up to 50 epochs were performed.

Using these paramters, various layer structures were tested. In order to avoid potentially overfitting, initial attempts did not increase the number of layers, but the number of units in the hidden layer. Trying a half as many units in the hidden layer as in the input layer, full-batch Gradient Descent converged quite well. Tweaking the batch size and learning rate with this structure, with a learning rate of $0.002$ and a batch size of 1000 (one tenth of the entire training set), the model was able to achieve $99.9\%$ training set accuracy and $94.4\%$ accuracy on the validation set within 20 epochs. Applying the model after 20 epochs to the testing set, the model was able to achieve $95.6\%$ accuracy.

\begin{figure} [H]
\begin{center}
\includegraphics[scale=0.5]{./images/q3_loss.png}
\caption{Training Set and Validation Set loss over epochs performed}
\end{center}
\end{figure}

\newpage
\section{Confusion Matrix}
To indentify where the model is performing poorly, a confusion matrix was created using the \verb+sklearn+ confusion matrix methods.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{./images/confusion}
\end{center}
\end{figure}

The model performs reasonably well, but there are a few misidentifications that the model makes more frequently than others. The most common mistakes the model makes is interpreting the digit 2 as the digit 7, the digit 8 as the digit 3, and the digit 4 as the digit 9. This is reasonable, since visually, these digits have similar features.

Ideally, to resolve this issue, obtaining more data points with these particular digits could be helpful. Assuming we did not have access to such data, some improvements that could be made to the model include:

\begin{enumerate}
\item performing more iterations of numerical optimization. Preliminary tests show the model is not too prone to overfitting, and that up to 30 epochs with the same model, the test and validation accuracies are marginally improved.
\item adjusting units in the hidden layer. Adding more units to the hidden layer would increase the number of features that can be interpreted by the model from the inputs, which could potentially help it distinguish between visually similar digits
\item increasing the number of hidden layers. While this runs the risk of overfitting, with the correct set of hyperparamters, a model with two hidden layers would be able to model a more complex underlying function to interpret the handwritten digits
\end{enumerate}


\end{document}








