\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 3}
\author{Daniel Han: 1006842534}
\date{March 2023}

\begin{document}

\maketitle

\section{Gradient Descent Variations}

\subsection{Setup/Mathematical Background}
We were tasked with implementing different variations of gradient descent with different hyperparameters to measure the performance of different methods across the hyperparameter grids for each using the \verb+pumadyn32nm+ dataset. The dataset, with 32-element inputs and scalar outputs, was used to train a linear model regression model with minimization of least-squares loss. Predictions for a given input would be calculated as the inner product of the model's weight vector and the input vector. The loss function to be minimized, in vectorized form, can be written as shown below, where $\mathbf{X}$ is the matrix whose rows are the inputs in the training dataset.

\begin{equation}
f(\mathbf{X}, mathbf{w}) = \| \mathbf{y} - \mathbf{X}\mathbf{w} \| = \mathbf{y}^{T} \mathbf{y} - 2\mathbf{w}^{T} \mathbf{X}^{T} \mathbf{y} - \mathbf{â€¢}bf{w}^{T} \mathbf{X}^{T} \mathbf{X}\mathbf{w}
\end{equation}

Previously, we solved for $\mathbf{w}$ analytically by setting the gradient of the loss function with respect to the weight vector and equating it to zero. This time, we will perform variations of gradient descent to find a numeric solution for the $\mathbf{w}$ vector. To do so, we will need to be able to calculate the gradient of the loss function with respect to the weight vector at any given iteration (i.e. for any given value of $\mathbf{w}$). While many other generic implementations of gradient descent optimization (such as numpy's built in optimization method) make use of a numeric approximation to the gradient, we are able to find the exact expression for the gradient at any iteration since we will always be finding the gradient for the same loss function. The gradient of the loss function is denoted $g(\mathbf{w})$.

\begin{equation}
\mathbf{g}(\mathbf{w}, \mathbf{X}) = -\mathbf{X}^T\mathbf{y} - \mathbf{X}^{T}\mathbf{X}\mathbf{y}
\end{equation}

For gradient descent, each iteration will update the weight vector using $\mathbf{w}_{k+1} = \mathbf{w}_{k} - \alpha \cdot \mathbf{g}(\mathbf{w}_{k}, \mathbf{X})$, where $\alpha$ is a non-negative learning rate. For the sake of this exercise, rather than imposing a well-defined stopping criterion for our gradient descent, each variation will be run for some number of iterations/epochs so that their relative performances over iterations/folds can be compared.

For the stochastic gradient descent variations, the same update step was applied, except rather than using the full input matrix $\mathbf{X}$ to compute the derivative, a subset of the rows in the input matrix form a matrix denoted $\mathbf{X'}$, whose row count is equal to the batch size specified. To add momentum to the gradient descent, the gradient of the loss function was computed using batches as before, but the vector used to update the weights was computed as the weighted average of the previous iteration's vector and the gradient of the loss function in the current iteration.

\subsection{Python Implementation}
Iterations of gradient descent and all auxiliary calculations were done in a single python object called \verb+graddesc+, which could iteratively update the weight vector of the model. A \verb+reset+ method was also added, which would reset the weight vector to allow multiple variations of gradient descent (using different hyperparameters) could be run without having to reinstantiate the class and re-import the entire dataset. The class also kept track of the squared loss at every single iteration to compare against the analytic solution to the least-squares loss problem. the analytic solution was computed using \verb+np.linalg.lstsq+ to find the pseudoinverse of the input matrix and find the vector $\mathbf{w}$ that would yield the least squared loss. A percent error threshold could also be defined for the gradient descent, indicating the allowable loss at which the gradient descent iterations would stop. However, in the code, for the sake of testing, the algorithm continues for a specified number of iterations even if the stopping criterion is met.

The assignment asked for plots over both time passed and epochs run.

\subsection{Results}
% find optimal hyperparameters for fast/stable convergence. Since everything runs prety quickly, get a relatively big/fine grid of hyperparamters, check plot for each, compare performance.
% once I've done that, augment code to plot not over iterations but over epochs and over runtime
% get plots and drop them in here
Before comparing the relative performances of each variation of gradient descent, the optimal hyperparamters that minimized convergence time without sacrificing too much stability were selected for each of the variations. For standard gradient descent, and stochastic gradient descent (given a fixed batch size), the only hyperparamter to test for was the learning rate. For SGD with momentum, however, tests were performed across a grid of hyperparamters with varying learning rates and momentum weight values $\beta$.

For standard gradient descent, the set of learning rate values were defined as $\alpha \in \{0.00005, 0.0001, 0.00025, 0.0005, 0.001\}$. Values of $\alpha$ greater than or equal to 0.00005 did not converge at all and had increasing error with more iterations. $\alpha = 0.0005$ converged to within 0.01\% of the analytic least-squares loss fastest, taking just two iterations. However, the larger $\alpha$ value would have made the model too sensitive to changes in the data set to be widely applicable, so the more conservative value of $\alpha = 0.00025$ was selected, which converged to within 0.01\% in just 7 iterations.

For Stochastic Gradient Descent, batch sizes of 1 and 10 were tried. To compare performance over epochs, since 100 iterations were run for full gradient descent, 100 epochs were run for each batch size to compare performance. Additionally, in order to weight every point in the dataset evenly, it was assumed that stochastic gradient descent would always be performed for a integer number of epochs (i.e. the program would not terminate mid-epoch). Trying the same range of values for the learning rate, a learning rate of 0.0001 yielded the fastest convergence while still being able to converge very closely to the analytically computed minimum loss. Then, adding momentum to SGD for a batch size of 1 using the learning rate from before, it was found that across a wide range of beta values that the convergence and steady-state error did not change significantly across different values of beta. However, while SGD yielded an error within 0.01\% of the minimum error for a linear model, with the addition of momentum, the SGD model was not able to reach 0.01\% error within 100 epochs, meaning it had a greater steady-state error.

In terms of computation time, all four methods with their optimal learning rates were able to converge in under ten epochs. However, when calculating the time it took per epoch over 100 epochs (not including the time it took to calculate the loss at each iteration), standard Gradient Descent was fastest, with all computations taking 0.02s. SGD with batch sizes of 1 and 10 took 1.21 and 0.14 seconds respectively to perform the same number of iterations. With momentum and a batch size of 1, the total computation time increased to 0.19 seconds for 100 epochs. Thus for this dataset, standard Gradient Descent converged as fast as other methods in terms of epochs required and faster than other methods in terms of computation time. It should be noted, however, that in general, for all variations, less computation was required for calculating the gradient at each step since an analytical expression for the derivative of the loss function was available, which may not always be the case in general for gradient descent.

In summary, while higher learning rates do yield faster convergence, they also tend to yield greater final errors from the analytical solution. Thus for each application, it is up to the user to determine an appropriate learning rate for the task at hand depending on how they value the runtime/accuracy tradeoff.

\section{Figures}
Plots of the loss of each variation of gradient descent (with different hyperparamaters) across epochs are provided below.

\begin{figure}
\centering
\includegraphics[scale=0.5]{./images/GD_5e-05}
\includegraphics[scale=0.5]{./images/GD_0.0001}
\includegraphics[scale=0.5]{./images/GD_0.00025}
\includegraphics[scale=0.5]{./images/GD_0.0005}
\includegraphics[scale=0.5]{./images/GD_0.001}
\caption{Loss across iterations for various Learning Rates for Gradient Descent}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{./images/SGD_batch1_5e-05}
\includegraphics[scale=0.5]{./images/SGD_batch1_0.0001}
\includegraphics[scale=0.5]{./images/SGD_batch1_0.00025}
\includegraphics[scale=0.5]{./images/SGD_batch1_0.0005}
\includegraphics[scale=0.5]{./images/SGD_batch1_0.001}
\caption{Loss across epochs for various Learning Rates for Stochastic Gradient Descent with batch size 1}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{./images/SGD_batch10_5e-05}
\includegraphics[scale=0.5]{./images/SGD_batch10_0.0001}
\includegraphics[scale=0.5]{./images/SGD_batch10_0.00025}
\includegraphics[scale=0.5]{./images/SGD_batch10_0.0005}
\includegraphics[scale=0.5]{./images/SGD_batch10_0.001}
\caption{Loss across epochs for various Learning Rates for Stochastic Gradient Descent with batch size 10}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{./images/SGD_mom_0.3}
\includegraphics[scale=0.5]{./images/SGD_mom_0.5}
\includegraphics[scale=0.5]{./images/SGD_mom_0.75}
\includegraphics[scale=0.5]{./images/SGD_mom_0.9}
\includegraphics[scale=0.5]{./images/SGD_mom_0.95}
\caption{Loss across epochs for Stochastic Gradient Descent with batch size 1 and varying beta}
\end{figure}

\end{document}