\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 4}
\author{Daniel Han: 1006842534}
\date{April 2023}

\begin{document}

\maketitle


\section{Data Partitioning}
The proposed data partitioning allocates 1001 data points for training, 200 for validation, and 300 for testing. While the proposed partitioning splits the data into reasonably sized proportions between the three subsets, the subsets are split chronologically, which could be disadvantageous, since it forces the model to train on the first 1001 time steps, and then the validation and test sets would just be measuring its ability to extrapolate beyond the time steps included in the training data.

It would be more appropriate to ensure that data points from various periods in the flow are included in each of the three data subsets. This can be done in a couple of ways. One could iterative assign each time step's measurement to one of the three sets, starting with time step 0 and working to the final time frame (i.e. assign 10 points to training, 2 points to validation, 3 points to testing, etc.). Alternatively, one could also randomize the ordering of the data points, then partition the reordered dataset as done previously. In any case, it would be beneficial to have a partitioning method that does not explicitly force the model to extrapolate to predict on the validation and test sets, and randomizing the ordering of the data points could improve the probability of each of the subsets having greater variety in the time step from which their data points were drawn.

\section{PCA of flow data}
\subsection{Determining PCA matrix for the dataset}
To perform a Gaussian Process analysis on a dataset with 25600 dimensions is not computationally viable for most parties not having access to a NASA supercomputer, which the author did not (at least at the time of writing this document). Thus, to save on computation without disregarding performance, a PCA could be performed to reduce the dimensionality of the dataset to a more manageable number. The number of reduced dimensions used was denoted $z_d$ (kept at a value of 4 for this assignment). To perform PCA, a vector $\mathbf{b} \in \mathbb{R}^{D}$ and a matrix $\mathbf{U} \in \mathbb{R}^{d \times D}$ were required to compute $\mathbf{z}$, the reduced dimension data points as $\mathbf{z} = \mathbf{U^T}(\mathbf{y} - \mathbf{b})$.

To minimize the reconstruction error and thereby keep the reduced data points as true as possible to the original data, one can select $\mathbf{b}$ to be equal to the mean of the original data points, then find the projection matrix from the covariance matrix of the 25600 original dimensions. Using the Courant-Fischer Theorem, the first $z_d$ principal components of the dataset corresponded to the $z_d$ eigenvectors of $\mathbf{\Sigma}$ with the largest corresponding eigenvalues, where $\Sigma$ is the covariance matrix of the dataset's variables. The matrix $\mathbf{U}$ can then be constructed by assigning its rows to be equal to the eigenvectors selected.

\subsection{Reconstruction with reduced dimensionality of 4}
To get a better qualitative undestanding of the four latent states, each of the four latent statesd was plotted over time to visualize the data.

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{latent_states_plot}
\caption{Plots of the four latent states over the flow sequence}
\end{center}
\end{figure}

\section{Element-wise predictions for the GP}
Having been provided code that could perform a GP prediction on a scalar input, we were tasked with extending this function to work for our multi-dimensional PCA representation of the original data. Denoting our reduced dimensionality as $D = 4$, we could simply treat each of the $D$ features as independent scalars and use the code provided to compute the posterior $\boldsymbol{\mu}$ and $\mathbf{\Sigma}$ for each of the four features. Then, stacking the resulting mean vectors and covariance matrices, we would have, in two matrices, our predictions for the distributions of each test point in succinct form.

The code provided was used without any significant changes to find the mean vector and covariance prediction for each of the latent states independently. A minor implementation change was made to reshape the x matrices only for the functions that required them. For instance, for the squared exponent kernel provided, a 1-D array inputs were reshaped to 2-D array inputs with the second axis being of length one. The code used in provided below.

\begin{verbatim}
def gp_pred_multidim(x : np.ndarray, y, x_test, kernel, noise_var = 1e-6):
    """
    we have four scalar targets (elements of y),
    each of which need to be tested across all time steps (time steps in x_test)
    perform iteratively using the same time steps every time, but a different column of y_test
    return stacked matrices of all four variables in one
    """
    print(y.shape)
    D = y.shape[1]
    N = x.shape[0]
    N_test = x_test.shape[0]
    x_test = x_test.reshape((-1, 1))
    list_mu = []
    list_cov = []
    for dim in range(D):
        # x, x_test are the same every time
        y_i = y[:,dim] # extract only the colummn of i we want
        y_i = y_i.reshape((-1,1))
        
        C = cho_factor(kernel(x, x) + noise_var*np.identity(N))
        
        mu_pred = kernel(x_test, x).dot(cho_solve(C, y_i))
        list_mu.append(mu_pred)
        
        cov_pred = (
            kernel(x_test, x_test) + noise_var*np.identity(N_test)
            - kernel(x_test, x.reshape((-1,1))).dot(cho_solve(C, kernel(x.reshape((-1,1)), x_test)))
        )
        list_cov.append(cov_pred)
    
    mu = np.hstack(list_mu)
    cov = np.stack(list_cov, axis = 2)
    print("shape of mean covariance matrices: {}, {}".format(mu.shape, cov.shape))
    return mu, cov
\end{verbatim}


\section{Element-wise log evidence}
Following the same procedure as was done for the covariance and mean predictions, the provided \verb+gp_evidence+ function was modified to deal with multidimensional targets. The modified function definition is shown below.

\begin{verbatim}
def gp_ev_multidim(x, y, kernel, noise_var = 1e-6):
    N = x.shape[0]
    C = cho_factor(kernel(x, x) + noise_var*np.identity(N))    
    D = y.shape[1]
    logs = []
    for dim in range(D):
        y_i = y[:,dim]
        y_i = y_i.reshape((-1,1))
        
        log_i = (
        0.5*y_i.T.dot(cho_solve(C,y))
        -np.sum(np.log(np.diag(C[0])))
        - 0.5*N*np.log(2*np.pi)
        )
        
        logs.append(log_i)
    log_evidence = np.vstack(logs)
    return log_evidence
\end{verbatim}

\section{Performing Type-II Inference}


















\end{document}