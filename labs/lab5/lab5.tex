\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 4}
\author{Daniel Han: 1006842534}
\date{April 2023}

\begin{document}

\maketitle


\section{Data Partitioning}
The proposed data partitioning allocates 1001 data points for training, 200 for validation, and 300 for testing. While the proposed partitioning splits the data into reasonably sized proportions between the three subsets, the subsets are split chronologically, which could be disadvantageous, since it forces the model to train on the first 1001 time steps, and then the validation and test sets would just be measuring its ability to extrapolate beyond the time steps included in the training data.

It would be more appropriate to ensure that data points from various periods in the flow are included in each of the three data subsets. This can be done in a couple of ways. One could iterative assign each time step's measurement to one of the three sets, starting with time step 0 and working to the final time frame (i.e. assign 10 points to training, 2 points to validation, 3 points to testing, etc.). Alternatively, one could also randomize the ordering of the data points, then partition the reordered dataset as done previously. In any case, it would be beneficial to have a partitioning method that does not explicitly force the model to extrapolate to predict on the validation and test sets, and randomizing the ordering of the data points could improve the probability of each of the subsets having greater variety in the time step from which their data points were drawn.

\section{PCA of flow data}
\subsection{Determining PCA matrix for the dataset}
To perform a Gaussian Process analysis on a dataset with 25600 dimensions is not computationally viable for most parties not having access to a NASA supercomputer, which the author did not (at least at the time of writing this document). Thus, to save on computation without disregarding performance, a PCA could be performed to reduce the dimensionality of the dataset to a more manageable number. The number of reduced dimensions used was denoted $z_d$ (kept at a value of 4 for this assignment). To perform PCA, a vector $\mathbf{b} \in \mathbb{R}^{D}$ and a matrix $\mathbf{U} \in \mathbb{R}^{d \times D}$ were required to compute $\mathbf{z}$, the reduced dimension data points as $\mathbf{z} = \mathbf{U^T}(\mathbf{y} - \mathbf{b})$.

To minimize the reconstruction error and thereby keep the reduced data points as true as possible to the original data, one can select $\mathbf{b}$ to be equal to the mean of the original data points, then find the projection matrix from the covariance matrix of the 25600 original dimensions. Using the Courant-Fischer Theorem, the first $z_d$ principal components of the dataset corresponded to the $z_d$ eigenvectors of $\mathbf{\Sigma}$ with the largest corresponding eigenvalues, where $\Sigma$ is the covariance matrix of the dataset's variables. The matrix $\mathbf{U}$ can then be constructed by assigning its rows to be equal to the eigenvectors selected.

\subsection{Reconstruction with reduced dimensionality of 4}
hahaha
% TODO: get plots of each element of z over time

\section{Element-wise predictions for the GP}
Having been provided code that could perform a GP prediction on a scalar input, we were tasked with extending this function to work for our multi-dimensional PCA representation of the original data. Denoting our reduced dimensionality as $D = 4$, we could simply treat each of the $D$ features as independent scalars and use the code provided to compute the posterior $\boldsymbol{\mu}$ and $\mathbf{\Sigma}$ for each of the four features. Then, stacking the resulting mean vectors and covariance matrices, we would have, in two matrices, our predictions for the distributions of each test point in succinct form.


\end{document}