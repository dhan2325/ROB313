\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 1}
\author{Daniel Han: 1006842534}
\date{Feburary 2023}

\begin{document}

\maketitle


\section{Thikonov Regularization}
To find the $\textbf{w}$ that minimizes the Tikhonov regularized cost, we can follow a similar procedure as in the course notes and take the partial derivative of the regularized cost with respect to the cost vector, and solve for the weight vector when the partial derivative has all components set to zero. To make the process easier, the optimization problem can be rewritten in vector terms as shown below:

\begin{equation}
\argmin_{\textbf{w} \in \mathbb{R}} \;\;  (\| \textbf{y} - \mathbf{\Phi}\mathbf{w} \|_2^2 + \mathbf{w^T}\mathbf{\Gamma}\mathbf{w})
\end{equation}

Where the expression which ic being minimized is the regularized cost written in vectoral form. Taking the derviative of this cost with respect to the weight vector, the following expression is derived:

\begin{equation}
\mathbf{\Phi^T}\mathbf{y} = (\mathbf{\Phi^T \Phi} + \mathbf{\Gamma})\mathbf{w}
\end{equation}

As one may have expected, the expression is very similar to the expression derived in class to solve for the weight vector using a regularized cost, except the lambda term is replaced by a $\mathbf{\Gamma}$. When the premultiplying term on $\mathbf{w}$ is invertible, there is a unique weight vector $\mathbf{w}_{min}$ that will minimize the regularized cost. If the matrix is non-invertible, a unique solution does not exist and there may exist multiple weight vectors that will yield the minimum regularized cost. However, $\boldsymbol\Gamma$ is PSD (positive semi-definite), and since $\boldsymbol\Phi$ is symmetric, $\boldsymbol\Phi^T \boldsymbol\Phi$ must also bs PSD. Since the sum of PSD matrices is also PSD, the premultiplicative matrix is guaranteed to have an inverse, meaning the unique solution for $\mathbf{w}$ can be expressed as:

\begin{equation}
\mathbf{w} = (\mathbf{\Phi^T \Phi} + \mathbf{\Gamma})^{-1}\mathbf{\Phi^T}\mathbf{y}
\end{equation}

For which the inverse can be computed using a singular value decomposition.

\section{Minimizing Objective Function}
In class, an expression for $\boldsymbol\alpha$ was found using the dual representation in which the loss function was rewritten entirely as a function of $\boldsymbol\alpha$ and $\mathbf{\Phi}$ by expressing the weight vector, $\mathbf{w}$ in terms of $\boldsymbol\alpha$. For the sake of comparison, the values of $\boldsymbol\alpha$ can be estimated by minimizing an objective function written in terms of the vector $\boldsymbol\alpha$ and the kernel terms directly. The regularized cost function to be optimized in terms of $\boldsymbol\alpha$ and the kernel can be written in vectoral form as shown below:

\begin{equation}
\argmin_{\boldsymbol\alpha \in \mathbb{R}^N} \;\; ( (\mathbf{y} - \mathbf{K}\boldsymbol\alpha)^T (\mathbf{y} - \mathbf{K}\boldsymbol\alpha) + \lambda \boldsymbol\alpha^T \mathbf{I} \boldsymbol\alpha)
\end{equation}

The objective function can be differentiated with respect to the input vector $\boldsymbol\alpha$, with each component set to zero. For the sake of partial differentiation, the terms in the kernel vector $\mathbf{k}$ are treated as being indeependent on the terms of $\boldsymbol\alpha$, despite both being functions of $\boldsymbol\phi$, the feature vector. Taking the partial derivative as mentioned and equating it to the zero vector yields the following equation (after some rearranging):

\begin{equation}
(\mathbf{K} \mathbf{K} + \lambda \mathbf{I}) \boldsymbol\alpha = \mathbf{Ky}
\end{equation}

If the matrix premultiplying alpha can be proven to be invertible, then the value $\boldsymbol\alpha$ can be determined by using one of our numeric methods for determining the inverse of the premultiplicative matrix. Since the gram matric is symmetric, the premultiplicative matrix can be expressed as the sum of two PSD matices, guaranteeing invertibility. With invertibility guaranteed, any general inversion method would work for the given matrix (i.e. SVD), and the solution for $\boldsymbol\alpha$ can be expressed as below:

\begin{equation}
\boldsymbol\alpha = (\mathbf{KK} + \lambda \mathbf{I})^{-1}\mathbf{Ky}
\end{equation}

The expression is different than the one obtained with the dual formulation. The expression obtained above treats each of the kernel evaluations as one of $N$ features, whose weights are being optimized for (with an added regularization $\lambda$ term). In the Dual formation, the weights being regularized are associated with features in the Reproducing Kernel Hilbert Space.

\iftrue
\section{Radial Basis Function with Gaussian Kernel}
For the third question, we are forming a Radial Basis Function regression model, (RBF), for which the predictions can be expressed in terms of a kernel $k$ and the alpha vector $\boldsymbol\alpha$:

\begin{equation}
\hat{f}(\mathbf{x}, \boldsymbol\alpha) = \sum_{i=1}^{N} \alpha_i k(\mathbf{x}, \mathbf{x}^{(i)})
\end{equation}


\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{kernels}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

The model was to be implemented in Python using simple numpy libraries. For the application at hand, an Isotropic Gaussian Kernel was selected. Since kernel evaluations were done not by individual terms but only to compute en entire gram matrix, vectorization with numpy was used to make the process more efficient. In order to solve for the value of alpha that would minimize the least-squares loss, the value of $\boldsymbol\alpha$ can determined by setting the gradient of the loss function to zero and solving for the values of $\boldsymbol\alpha$ that satisfy the equation. The following expression for $\boldsymbol\alpha$ can be derived:

\begin{equation}
\boldsymbol\alpha = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}
\end{equation}

The coefficient for regularization, $\lambda$, should always be positive to increase the cost of having terms of large magnitude in the weighting vector. Given then that $\lambda$ is positive, the matrix $\mathbf{K} + \lambda \mathbf{I}^{-1}$ is guaranteed to be positive semi-definite which in turn implies that the matrix is Hermitian. The inverse of a Hermitian matrix can be computed using the Cholesky factorization, which can be performed on a numpy ndarray using \verb+scipy.linalg.cho_factor+. The Cholesky factorization can express the matrix in terms of a lower triangular matrix and its conjugate transpose (denoted with the $^*$ operator), for which it is much simpler to compute the inverse of a matrix. Letting $\mathbf{L}$ be a lower diagonal matrix,

\begin{equation}
(\mathbf{K} + \lambda \mathbf{I}) = \mathbf{L} \mathbf{L}^*
\end{equation}
\begin{equation}
(\mathbf{K} + \lambda \mathbf{I})^{-1} = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1}
\end{equation}
\begin{equation}
\boldsymbol\alpha = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1} \mathbf{y}
\end{equation}

Thus, an expression for $\boldsymbol\alpha$ is obtained in terms of the desired output $\mathbf{y}$ and the inverse of the Cholesky decomposition of the matrix $\mathbf{K} + \lambda \mathbf{I})$, bypassing the need to perform a direct inversion, which could be more computationally expensive. Conveniently, the scipy library also has a method \verb+scipy.linalg.cho_solve+ which can solve a system of linear equations using the lower matrix determined from the Cholesky factorization. As such, a direct inverse is never computed, but nonetheless, it is nice to be assured that it will always exist for $\lambda \neq 0$

The method consists of four key steps:
\begin{enumerate}
\item build an $N \times N$ gram matrix from the training data using the Gaussian Kernel
\item solve for the Cholseky decomposition (using the gram matrix we just computed) necessary to compute $\boldsymbol\alpha$, and use it to solve for $\boldsymbol\alpha$
\item find a new gram matrix using the same method as before, but now, use the test data as the first input ($x$) and the training data as the second input ($z$
\item predict on the test set by taking the dot product of the second gram matrix with the testing data
\end{enumerate}

Then, the overall RMSE for the predictions for validation and testing could be computed with the $numpy.linalg.norm$ method. The code required to complete the tasks is shown below:


\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{rbf_def}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

Using the RBF class and the \verb+run_validation+ class method, the RMSE loss across the dataset was found for various values of $\theta$ and $\lambda$ and the results written to a text file. The code for running validation and the output for both \verb+mauna_loa+ and \verb+rosenbrock+ are shown below.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{validation}
\caption{the code used to evaluate RMSE loss across the grid of hyperparameters}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & $\theta$ = 0.05 & 0.1 & 0.5 & 1 & 2 \\ \hline
 $\lambda$ = 0.001  & 1.2917 & 1.4163 & 0.3471 & \textbf{0.1245} & 0.2017\\ \hline
 0.01 & 1.1173 & 1.0591 & 0.4277 & 0.2295 & 0.2524\\ \hline
 0.1 & 1.082 & 0.9659 & 0.4737 & 0.3391 & 0.2171\\ \hline
 1 & 1.0992 & 0.9967 & 0.6063 & 0.4436 & 0.2492\\
 \hline
\end{tabular}
\caption{RMSE cross the validation set for mauna\_loa using different hyperparameters}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & $\theta$ = 0.05 & 0.1 & 0.5 & 1 & 2 \\ \hline
 $\lambda$ = 0.001  & 0.7355 & 0.6266 & 0.3515 & 0.2572 & \textbf{0.1932}\\ \hline
 0.01 & 0.7389 & 0.632 & 0.381 & 0.2974 & 0.241\\ \hline
 0.1 & 0.7523 & 0.6477 & 0.4191 & 0.3582 & 0.3117\\ \hline
 1 & 0.8081 & 0.7205 & 0.5133 & 0.4666 & 0.4365\\
 \hline
\end{tabular}
\caption{RMSE cross the validation set for rosenbrock using different hyperparameters}
\end{center}
\end{table}

For both datasets, the optimal value of $\lambda$ was 0.001, the smallest value tested for. This suggests that increasing the regularization factor worsens the model, and that our model is not particularly prone to overfitting to the datasets that we tested it with. For \verb+mauna_loa+, $\theta = 1$ yielded the lowest RMSE, and for \verb+rosenbrock+ a value of $\theta = 2$ worked best. Using these hyperparamters to predict on the test set, the model yielded an RMSE error of 0.150 for \verb+mauna_loa+ and 0.185 for \verb+rosenbrock+


\pagebreak
\fi
\section{Orthogonal Matching Pursuit}
For the fourth question, we were tasked with implementing a greedy algorithm using a set of basis functions of our own design. In order to make justifiable decisions regarding the selection of basis functions, the \verb+mauna_loa+ dataset was plotted in order to make observations about the dataset (plot provided below). In order to form a set of basis functions, it was assumed that there was some underlying function for the dataset, and that each data point was sampled from this underlying function (with the addition of some error). With this assumption, by observing the data, one could presumably construct a numeric function from a set of basis functions that closely resembles the underlying function, rendering our model effective in interpolation and extrapolation.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{mauna_loa}
\caption{a plot of the training and validation datapoints for mauna\_loa}
\end{figure}

From the plot, a number of observations/claims could be made about the underlying pattern that produced the datapoints as they were observed. Firstly, the evidence shows strongly that there is some sinusoidal/periodic component to the underlying function. Additionally, the average value of the function appears to increase periods. Moreover, the average value across a period seems to increase monotonically at a rate that is somewhat faster than linear (convex). Finally, the magnitude of the oscillations appears to be relatively steady, suggesting that the periodic term and the monotonically increasing terms could be added as opposed to multiplied in the underlying function for the \verb+mauna_loa+ dataset.

From these observations, it made sense to construct a dictionary of basis functions from a set of function classes, and include in the dictionary several permutations from each function class. Each function class could be expressed mathematically as a function of the `input' of the dataset (which in the context of \verb+mauna_loa+ is a date) and a few parameters that would be set differently for each permutation of the function in the dictionary. The different function classes used are explained below with their respective parameters:

\begin{itemize}
\item $y=A * \sin(\omega x - \phi)$, a sinusoid with parameters for angular frequency, horizontal offset
\item $ y=mx$, a linear function with a slope parameter
\item $ y=a (x - b)^2$, a quadratic function with a vertical scaling and a horizontal offset
\item $ y=d$, a constant vertical offset
\end{itemize}

Since the weight vector would be determined so that the vertical scaling of each basis function would produce the least squares, paramters for the vertical stretch of the functions could end up being redundant (add an unnceessary degree of freedom), but did not affect the results. A python class was formulated to differentiate between the various basis functions.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{basis_class}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

With a dictionary of basis functions well-defined, the OMP algorithm could be applied. The $k^{th}$ iteration of the algorithm followed the steps outlined below:

\begin{itemize}
\item pick a new basis function from the list of unused basis functions with the greatest $J$ metric
\item add the selected basis function to the list of basis functions in use, and remove it from the list of basis functions being used
\item solve for the new weight vector. Since the $\Phi$ matrix was not full rank, the pseudoinverse of $\Phi$ was determined to find the weighting of each basis function that would minimize the least-squares loss of the prediction.
\item with the new basis and weights, determine the new residual and iterate until the stopping criterion is begins to increase in value (adding new features results in overfitting)
\end{itemize}

The expression for the metric $J$ is provided below:
\begin{equation}
J(\phi_i) = \frac{(\mathbf{\Phi}(:,i)^T\mathbf{r}^{(k)})^2}{\mathbf{\Phi}(:,i)^T\mathbf{\Phi}(:,i)}
\end{equation}

The dictionary of basis functions was created by defining ranges for each parameter for each of the basis functions. Each parameters was assigned a minimum and maximum values along with a step size parameter determining how finely to sample for the given paramter. The function for creating the initial basis is depicted below. Additionally, a python class was created for the OMP aglorithm, to store  information about the algorithm (iteration number $k$, updated matrix $\Phi$, etc.), and methods were defined to run an iteration of OMP and to select a candidate from the candidacy list.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{build_basis}
\caption{function used to add functions to the basis using a grid of paramters for each function class}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{omp_class}
\caption{class defined to efficiently and cleanly run the OMP optimization}
\end{figure}

The optimization algorithm ended up selecting 7 functions from the basis of over 400. The algorithm begins be selecting a linear function, then a a sinusoidal to model the gener periodic and increasing trend of the data. Then, the algorithm selects a quadratic function to account for the non-linear growth of the amplitude and a horizontal to counter the offset introduced by the quadratic. Finally, the algorithm adds a few more sinusoidal terms and stops the iteration after 7 functions have been added.

Using the computed features and weights to predict on the test set, it was noted that the selected function appeared to model the trend with less accuracy when extrapolating (all test data points were extrapolations as opposed to interpolations). Still, the test set predictions had an RMSE value of 0.056, indicating the model performs reasonably well.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{test_meth}
\caption{OMP object method for predicting on the test dataset and determining RMSE}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.37]{omp_test}
\caption{OMP model's extrapolation vs the test dataset}
\end{figure}

\end{document}