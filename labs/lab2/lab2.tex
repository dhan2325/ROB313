\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 1}
\author{Daniel Han: 1006842534}
\date{Feburary 2023}

\begin{document}

\maketitle


\section{Thikonov Regularization}
To find the $\textbf{w}$ that minimizes the Tikhonov regularized cost, we can follow a similar procedure as in the course notes and take the partial derivative of the regularized cost with respect to the cost vector, and solve for the weight vector when the partial derivative has all components set to zero. To make the process easier, the optimization problem can be rewritten in vector terms as shown below:

\begin{equation}
\argmin_{\textbf{w} \in \mathbb{R}} \;\;  (\| \textbf{y} - \mathbf{\Phi}\mathbf{w} \|_2^2 + \mathbf{w^T}\mathbf{\Gamma}\mathbf{w})
\end{equation}

Where the expression which ic being minimized is the regularized cost written in vectoral form. Taking the derviative of this cost with respect to the weight vector, the following expression is derived:

\begin{equation}
\mathbf{\Phi^T}\mathbf{y} = (\mathbf{\Phi^T \Phi} + \mathbf{\Gamma})\mathbf{w}
\end{equation}

As one may have expected, the expression is very similar to the expression derived in class to solve for the weight vector using a regularized cost, except the lambda term is replaced by a $\mathbf{\Gamma}$. When the premultiplying term on $\mathbf{w}$ is invertible, there is a unique weight vector $\mathbf{w}_{min}$ that will minimize the regularized cost. If the matrix is non-invertible, a unique solution does not exist and there may exist multiple weight vectors that will yield the minimum regularized cost.

\section{Minimizing Objective Function}
In class, an expression for $\boldsymbol\alpha$ was found using the dual representation in which the loss function was rewritten entirely as a function of $\boldsymbol\alpha$ and $\mathbf{\Phi}$ by expressing the weight vector, $\mathbf{w}$ in terms of $\boldsymbol\alpha$. For the sake of comparison, the values of $\boldsymbol\alpha$ can be estimated by minimizing an objective function written in terms of the vector $\boldsymbol\alpha$ and the kernel terms directly. The regularized cost function to be optimized in terms of $\boldsymbol\alpha$ and the kernel can be written in vectoral form as shown below:

\begin{equation}
\argmin_{\boldsymbol\alpha \in \mathbb{R}^N} \;\; ( (\mathbf{y} - \mathbf{K}\boldsymbol\alpha)^T (\mathbf{y} - \mathbf{K}\boldsymbol\alpha) + \lambda \boldsymbol\alpha^T \mathbf{I} \boldsymbol\alpha)
\end{equation}

The objective function can be differentiated with respect to the input vector $\boldsymbol\alpha$, with each component set to zero. For the sake of partial differentiation, the terms in the kernel vector $\mathbf{k}$ are treated as being indeependent on the terms of $\boldsymbol\alpha$, despite both being functions of $\boldsymbol\phi$, the feature vector. Taking the partial derivative as mentioned and equating it to the zero vector yields the following equation (after some rearranging):

\begin{equation}
(\mathbf{K} \mathbf{K} + \lambda \mathbf{I}) \boldsymbol\alpha = \mathbf{Ky}
\end{equation}

If the matrix premultiplying alpha can be proven to be invertible, then the value $\boldsymbol\alpha$ can be determined by using one of our numeric methods for determining the inverse of the premultiplicative matrix. Since the gram matric is symmetric, the premultiplicative matrix can be expressed as the sum of two PSD matices, guaranteeing invertibility. With invertibility guaranteed, any general inversion method would work for the given matrix (i.e. SVD), and the solution for $\boldsymbol\alpha$ can be expressed as below:

\begin{equation}
\boldsymbol\alpha = (\mathbf{KK} + \lambda \mathbf{I})^{-1}\mathbf{Ky}
\end{equation}

\iftrue
\section{Radial Basis Function with Gaussian Kernel}
For the third question, we are forming a Radial Basis Function regression model, (RBF), for which the predictions can be expressed in terms of a kernel $k$ and the alpha vector $\boldsymbol\alpha$:

\begin{equation}
\hat{f}(\mathbf{x}, \boldsymbol\alpha) = \sum_{i=1}^{N} \alpha_i k(\mathbf{x}, \mathbf{x}^{(i)})
\end{equation}


\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{kernels}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

The model was to be implemented in Python using simple numpy libraries. For the application at hand, an Isotropic Gaussian Kernel was selected. Since kernel evaluations were done not by individual terms but only to compute en entire gram matrix, vectorization with numpy was used to make the process more efficient. In order to solve for the value of alpha that would minimize the least-squares loss, the value of $\boldsymbol\alpha$ can determined by setting the gradient of the loss function to zero and solving for the values of $\boldsymbol\alpha$ that satisfy the equation. The following expression for $\boldsymbol\alpha$ can be derived:

\begin{equation}
\boldsymbol\alpha = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}
\end{equation}

The coefficient for regularization, $\lambda$, should always be positive to increase the cost of having terms of large magnitude in the weighting vector. Given then that $\lambda$ is positive, the matrix $\mathbf{K} + \lambda \mathbf{I}^{-1}$ is guaranteed to be positive semi-definite which in turn implies that the matrix is Hermitian. The inverse of a Hermitian matrix can be computed using the Cholesky factorization, which can be performed on a numpy ndarray using \verb+scipy.linalg.cho_factor+. The Cholesky factorization can express the matrix in terms of a lower triangular matrix and its conjugate transpose (denoted with the $^*$ operator), for which it is much simpler to compute the inverse of a matrix. Letting $\mathbf{L}$ be a lower diagonal matrix,

\begin{equation}
(\mathbf{K} + \lambda \mathbf{I}) = \mathbf{L} \mathbf{L}^*
\end{equation}
\begin{equation}
(\mathbf{K} + \lambda \mathbf{I})^{-1} = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1}
\end{equation}
\begin{equation}
\boldsymbol\alpha = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1} \mathbf{y}
\end{equation}

Thus, an expression for $\boldsymbol\alpha$ is obtained in terms of the desired output $\mathbf{y}$ and the inverse of the Cholesky decomposition of the matrix $\mathbf{K} + \lambda \mathbf{I})$, bypassing the need to perform a direct inversion, which could be more computationally expensive. Conveniently, the scipy library also has a method \verb+scipy.linalg.cho_solve+ which can solve a system of linear equations using the lower matrix determined from the Cholesky factorization. As such, a direct inverse is never computed, but nonetheless, it is nice to be assured that it will always exist for $\lambda \neq 0$

The method consists of four key steps:
\begin{enumerate}
\item build an $N \times N$ gram matrix from the training data using the Gaussian Kernel
\item solve for the Cholseky decomposition (using the gram matrix we just computed) necessary to compute $\boldsymbol\alpha$, and use it to solve for $\boldsymbol\alpha$
\item find a new gram matrix using the same method as before, but now, use the test data as the first input ($x$) and the training data as the second input ($z$
\item predict on the test set by taking the dot product of the second gram matrix with the testing data
\end{enumerate}

Then, the overall RMSE for the predictions for validation and testing could be computed with the $numpy.linalg.norm$ method. The code required to complete the tasks is shown below:


\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{rbf_def}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

Using the RBF class and the \verb+run_validation+ class method, the RMSE loss across the dataset was found for various values of $\theta$ and $\lambda$ and the results written to a text file. The code for running validation and the output for both \verb+mauna_loa+ and \verb+rosenbrock+ are shown below.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{validation}
\caption{the code used to evaluate RMSE loss across the grid of hyperparameters}
\end{figure}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & $\theta$ = 0.05 & 0.1 & 0.5 & 1 & 2 \\ \hline
 $\lambda$ = 0.001  & 1.2917 & 1.4163 & 0.3471 & \textbf{0.1245} & 0.2017\\ \hline
 0.01 & 1.1173 & 1.0591 & 0.4277 & 0.2295 & 0.2524\\ \hline
 0.1 & 1.082 & 0.9659 & 0.4737 & 0.3391 & 0.2171\\ \hline
 1 & 1.0992 & 0.9967 & 0.6063 & 0.4436 & 0.2492\\
 \hline
\end{tabular}
\caption{RMSE cross the validation set for mauna\_loa using different hyperparameters}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & $\theta$ = 0.05 & 0.1 & 0.5 & 1 & 2 \\ \hline
 $\lambda$ = 0.001  & 0.7355 & 0.6266 & 0.3515 & 0.2572 & \textbf{0.1932}\\ \hline
 0.01 & 0.7389 & 0.632 & 0.381 & 0.2974 & 0.241\\ \hline
 0.1 & 0.7523 & 0.6477 & 0.4191 & 0.3582 & 0.3117\\ \hline
 1 & 0.8081 & 0.7205 & 0.5133 & 0.4666 & 0.4365\\
 \hline
\end{tabular}
\caption{RMSE cross the validation set for rosenbrock using different hyperparameters}
\end{center}
\end{table}

For both datasets, the optimal value of $\lambda$ was 0.001, the smallest value tested for. This suggests that increasing the regularization factor worsens the model, and that our model is not particularly prone to overfitting to the datasets that we tested it with. For \verb+mauna_loa+, $\theta = 1$ yielded the lowest RMSE, and for \verb+rosenbrock+ a value of $\theta = 2$ worked best. Using these hyperparamters to predict on the test set, the model yielded an RMSE error of 0.150 for \verb+mauna_loa+ and 0.185 for \verb+rosenbrock+


\pagebreak
\fi
\section{Basis Function Dictionary}
For the fourth question, we were tasked with implementing a greedy algorithm using a set of basis functions of our own design. In order to make justifiable decisions regarding the selection of basis functions, the \verb+mauna_loa+ dataset was plotted in order to make observations about the dataset (plot provided below). In order to form a set of basis functions, it was assumed that there was some underlying function for the dataset, and that each data point was sampled from this underlying function (with the addition of some error). With this assumption, by observing the data, one could presumably construct a numeric function from a set of basis functions that closely resembles the underlying function, rendering our model effective in interpolation and extrapolation.

% provide figure here!!!!

From the plot, a number of observations/claims could be made about the underlying pattern that produced the datapoints as they were observed. Firstly, the evidence shows strongly that there is some sinusoidal/periodic component to the underlying function. Additionally, the average value of the function appears to increase periods. Moreover, the average value across a period seems to increase monotonically at a rate that is somewhat faster than linear (convex). Finally, the magnitude of the oscillations appears to be relatively steady, suggesting that the periodic term and the monotonically increasing terms could be added as opposed to multiplied in the underlying function for the \verb+mauna_loa+ dataset.

From these observations, it made sense to construct a dictionary of basis functions from a set of function classes, and include in the dictionary several permutations from each function class. Each function class could be expressed mathematically as a function of the `input' of the dataset (which in the context of \verb+mauna_loa+ is a date) and a few parameters that would be set differently for each permutation of the function in the dictionary. Firstly, to capture the periodic component of the underlying function, it was decided that the first class of functions used would be sinusoidal functions, with permutations varying in amplitude ($a$), frequency ($\omega$), and phase offset ($\phi$). Then, to capture the growth in average value across periods, a few different classes were added: a linear class of functions taking a slope parameter ($m$), an exponential class of functions with parameters, vertical scaling factor ($B$) and horizontal compression factor ($c$).
Finally, rather than including parameters for vertical offset in each function classes, since the vertical offsets in each basis function would add anyways, they were omitted from individual classes and rather the final function class used was the class of linear functions with zero slope, that took a single vertical offset parameter ($d$). The four classes are denoted with $f_i(x)$ and are expressed in terms of their parameters below.

\begin{equation}
f_1(x) = A \cdot \sin(\omega x - \phi)
\end{equation}
\begin{equation}
f_2(x) = m x
\end{equation}
\begin{equation}
f_3(x) = B \cdot e^{cx}
\end{equation}
\begin{equation}
f_4(x) = d
\end{equation}

With a dictionary of basis functions well-defined, the OMP algorithm could be applied. The $k^{th}$ iteration of the algorithm followed the steps outlined below:

\begin{itemize}
\item pick a new basis function from the list of unused basis functions with the greatest $J$ metric
\item add the selected basis function to the list of basis functions in use, and remove it from the list of basis functions being used
\item solve for the new weight vector. Since the $\Phi$ matrix was not full rank, the pseudoinverse of $\Phi$ was determined to find the weighting of each basis function that would minimize the least-squares loss of the prediction.
\item with the new basis and weights, determine the new residual and iterate until the residual is below a certain threshold defined by $\epsilon$
\end{itemize}

The expression for the metrioc $J$ is provided below:
\begin{equation}
J(\phi_i) = \frac{(\mathbf{\Phi}(:,i)^T\mathbf{r}^{(k)})^2}{\mathbf{\Phi}(:,i)^T\mathbf{\Phi}(:,i)}
\end{equation}

In terms of implementation in \verb+python3+, I defined a basis object to make the code simpler to read. Then, by inspection, I determined the range I suspected the true value of each parameter should be, and created a basis of functions across the range. For the first run, to save runtime, the exponenetial family of functions was left out. Even then, the first dictionary has 1259 basis functions in it.

\end{document}