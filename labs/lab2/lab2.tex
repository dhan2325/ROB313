\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 1in}

\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 1}
\author{Daniel Han: 1006842534}
\date{Feburary 2023}

\begin{document}

\maketitle

\section*{Question 1}
To find the $\textbf{w}$ that minimizes the Tikhonov regularized cost, we can follow a similar procedure as in the course notes and take the partial derivative of the regularized cost with respect to the cost vector, and solve for the weight vector when the partial derivative has all components set to zero. To make the process easier, the optimization problem can be rewritten in vector terms as shown below:

\begin{equation}
\argmin_{\textbf{w} \in \mathbb{R}} \;\;  (\| \textbf{y} - \mathbf{\Phi}\mathbf{w} \|_2^2 + \mathbf{w^T}\mathbf{\Gamma}\mathbf{w})
\end{equation}

Where the expression which ic being minimized is the regularized cost written in vectoral form. Taking the derviative of this cost with respect to the weight vector, the following expression is derived:

\begin{equation}
\mathbf{\Phi^T}\mathbf{y} = (\mathbf{\Phi^T \Phi} + \mathbf{\Gamma})\mathbf{w}
\end{equation}

As one may have expected, the expression is very similar to the expression derived in class to solve for the weight vector using a regularized cost, except the lambda term is replaced by a $\mathbf{\Gamma}$. When the premultiplying term on $\mathbf{w}$ is invertible, there is a unique weight vector $\mathbf{w}_{min}$ that will minimize the regularized cost. If the matrix is non-invertible, a unique solution does not exist and there may exist multiple weight vectors that will yield the minimum regularized cost.

\section*{Question 2}
In class, an expression for $\boldsymbol\alpha$ was found using the dual representation in which the loss function was rewritten entirely as a function of $\boldsymbol\alpha$ and $\mathbf{\Phi}$ by expressing the weight vector, $\mathbf{w}$ in terms of $\boldsymbol\alpha$. For the sake of comparison, the values of $\boldsymbol\alpha$ can be estimated by minimizing an objective function written in terms of the vector $\boldsymbol\alpha$ and the kernel terms directly. The regularized cost function to be optimized in terms of $\boldsymbol\alpha$ and the kernel can be written in vectoral form as shown below:

\begin{equation}
\argmin_{\boldsymbol\alpha \in \mathbb{R}^N} \;\; (\| \mathbf{y} - \mathbf{k(x)}^T\boldsymbol\alpha \|_{2}^2 + \lambda \|\boldsymbol\alpha \|)
\end{equation}

The objective function can be differentiated with respect to the input vector $\boldsymbol\alpha$, with each component set to zero. For the sake of partial differentiation, the terms in the kernel vector $\mathbf{k}$ are treated as being indeependent on the terms of $\boldsymbol\alpha$, despite both being functions of $\boldsymbol\phi$, the feature vector. Taking the partial derivative as mentioned and equating it to the zero vector yields the following equation (after some rearranging):

\textit{I can't figure this derivative out rn I will get back to this one later tonight?}
\begin{equation}
haha
\end{equation}

\section*{Question 3}
For the third question, we are forming a Radial Basis Function regression model, (RBF), for which the predictions can be expressed in terms of a kernel $k$ and the alpha vector $\boldsymbol\alpha$:

\begin{equation}
\hat{f}(\mathbf{x}, \boldsymbol\alpha) = \sum_{i=1}^{N} \alpha_i k(\mathbf{x}, \mathbf{x}^{(i)})
\end{equation}

For the application at hand, a Gaussian kernel was selected. In order to solve for the value of alpha that would minimize the least-squares loss, the value of $\boldsymbol\alpha$ can determined by setting the gradient of the loss function to zero and solving for the values of $\boldsymbol\alpha$ that satisfy the equation. The following expression for $\boldsymbol\alpha$ can be derived:

\begin{equation}
\boldsymbol\alpha = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}
\end{equation}

The coefficient for regularization, $\lambda$, should always be positive to increase the cost of having terms of large magnitude in the weighting vector. Given then that $\lambda$ is positive, the matrix $\mathbf{K} + \lambda \mathbf{I})^{-1}$ is guaranteed to be positive semi-definite which in turn implies that the matrix is Hermitian. The inverse of a Hermitian matrix can be computed using the Cholesky factorization, which can be performed on a numpy ndarray using \verb+numpy.linalg.cholesky+. The Cholesky factorization can express the matrix in terms of a lower triangular matrix and its conjugate transpose (denoted with the $^*$ operator), for which it is much simpler to compute the inverse of a matrix. Letting $\mathbf{L}$ be a lower diagonal matrix,

\begin{equation}
(\mathbf{K} + \lambda \mathbf{I}) = \mathbf{L} \mathbf{L}^*
\end{equation}
\begin{equation}
(\mathbf{K} + \lambda \mathbf{I})^{-1} = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1}
\end{equation}
\begin{equation}
\boldsymbol\alpha = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1} \mathbf{y}
\end{equation}

Thus, an expression for $\boldsymbol\alpha$ is obtained in terms of the desired output $\mathbf{y}$ and the inverse of the Cholesky decomposition of the matrix $\mathbf{K} + \lambda \mathbf{I})$, bypassing the need to perform a direct inversion, which could be more computationally expensive.




\begin{lstlisting}
class RBF:

    def __init__(self, dataset: str, theta : float, reg_factor : float):
        self.theta = theta
        self.reg =reg_factor
        if dataset == 'mauna_loa':
            self.x_train, self.x_valid, self.x_test, self.y_train, self.y_valid, self.y_test = load_dataset(dataset)
        elif dataset == 'rosenbrock':
            self.x_train, self.x_valid, self.x_test, self.y_train, self.y_valid, self.y_test = load_dataset(dataset, n_train = 1000, d = 2)
        else:
            raise Exception("please enter a valid dataset (mauna_loa or rosenbrock)")
        
        # initialize fields arbitrarily, will be set later by setup functions
        self.K = None
        self.inv_matrix = None
        self.N = 0

        # setup functions
        self.find_K()
        self.find_inv_matrix()

    def run_algo(self):
        pass
    
    def find_alpha(self, y : np.ndarray):
        # TODO: find the inverted matrix to get from y to alpha!
        return np.matmul(self.inv_matrix, y)
    
    def find_K(self):
        self.N = np.shape(self.x_train)[0]
        print(type(self.N))
        self.K = np.zeros((self.N, self.N))
        for i in range(self.N):
            for j in range (self.N):
                self.K[i][j] = iso_gaussian(self.x_train[i], self.x_train[j], self.theta)
    
    def find_inv_matrix(self):
        to_invert = self.K + self.reg * np.identity(self.N)
\end{lstlisting}
\section*{Question 4}


\end{document}