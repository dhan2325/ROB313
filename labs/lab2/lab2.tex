\documentclass{article}
\usepackage[utf8]{inputenc}
\setlength{\parskip}{12pt}

\usepackage{geometry}
\geometry{margin = 0.7in}

\usepackage{float}
\usepackage{graphicx}
\graphicspath{{./images/}}

\usepackage{mathtools, amssymb, amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}


\title{ROB313: Assignment 1}
\author{Daniel Han: 1006842534}
\date{Feburary 2023}

\begin{document}

\maketitle

\iffalse
\section*{Question 1}
To find the $\textbf{w}$ that minimizes the Tikhonov regularized cost, we can follow a similar procedure as in the course notes and take the partial derivative of the regularized cost with respect to the cost vector, and solve for the weight vector when the partial derivative has all components set to zero. To make the process easier, the optimization problem can be rewritten in vector terms as shown below:

\begin{equation}
\argmin_{\textbf{w} \in \mathbb{R}} \;\;  (\| \textbf{y} - \mathbf{\Phi}\mathbf{w} \|_2^2 + \mathbf{w^T}\mathbf{\Gamma}\mathbf{w})
\end{equation}

Where the expression which ic being minimized is the regularized cost written in vectoral form. Taking the derviative of this cost with respect to the weight vector, the following expression is derived:

\begin{equation}
\mathbf{\Phi^T}\mathbf{y} = (\mathbf{\Phi^T \Phi} + \mathbf{\Gamma})\mathbf{w}
\end{equation}

As one may have expected, the expression is very similar to the expression derived in class to solve for the weight vector using a regularized cost, except the lambda term is replaced by a $\mathbf{\Gamma}$. When the premultiplying term on $\mathbf{w}$ is invertible, there is a unique weight vector $\mathbf{w}_{min}$ that will minimize the regularized cost. If the matrix is non-invertible, a unique solution does not exist and there may exist multiple weight vectors that will yield the minimum regularized cost.

\section*{Question 2}
In class, an expression for $\boldsymbol\alpha$ was found using the dual representation in which the loss function was rewritten entirely as a function of $\boldsymbol\alpha$ and $\mathbf{\Phi}$ by expressing the weight vector, $\mathbf{w}$ in terms of $\boldsymbol\alpha$. For the sake of comparison, the values of $\boldsymbol\alpha$ can be estimated by minimizing an objective function written in terms of the vector $\boldsymbol\alpha$ and the kernel terms directly. The regularized cost function to be optimized in terms of $\boldsymbol\alpha$ and the kernel can be written in vectoral form as shown below:

\begin{equation}
\argmin_{\boldsymbol\alpha \in \mathbb{R}^N} \;\; (\| \mathbf{y} - \mathbf{k(x)}^T\boldsymbol\alpha \|_{2}^2 + \lambda \|\boldsymbol\alpha \|)
\end{equation}

The objective function can be differentiated with respect to the input vector $\boldsymbol\alpha$, with each component set to zero. For the sake of partial differentiation, the terms in the kernel vector $\mathbf{k}$ are treated as being indeependent on the terms of $\boldsymbol\alpha$, despite both being functions of $\boldsymbol\phi$, the feature vector. Taking the partial derivative as mentioned and equating it to the zero vector yields the following equation (after some rearranging):

\textit{I can't figure this derivative out rn I will get back to this one later tonight?}
\begin{equation}
haha
\end{equation}

\section*{Question 3}
For the third question, we are forming a Radial Basis Function regression model, (RBF), for which the predictions can be expressed in terms of a kernel $k$ and the alpha vector $\boldsymbol\alpha$:

\begin{equation}
\hat{f}(\mathbf{x}, \boldsymbol\alpha) = \sum_{i=1}^{N} \alpha_i k(\mathbf{x}, \mathbf{x}^{(i)})
\end{equation}

The model was to be implemented in Python using simple numpy libraries. For the application at hand, an Isotropic Gaussian Kernel was selected. The evaluation of the kernel was done in Python using the following helper functions:


\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{kernels}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

 In order to solve for the value of alpha that would minimize the least-squares loss, the value of $\boldsymbol\alpha$ can determined by setting the gradient of the loss function to zero and solving for the values of $\boldsymbol\alpha$ that satisfy the equation. The following expression for $\boldsymbol\alpha$ can be derived:

\begin{equation}
\boldsymbol\alpha = (\mathbf{K} + \lambda \mathbf{I})^{-1} \mathbf{y}
\end{equation}

The coefficient for regularization, $\lambda$, should always be positive to increase the cost of having terms of large magnitude in the weighting vector. Given then that $\lambda$ is positive, the matrix $\mathbf{K} + \lambda \mathbf{I})^{-1}$ is guaranteed to be positive semi-definite which in turn implies that the matrix is Hermitian. The inverse of a Hermitian matrix can be computed using the Cholesky factorization, which can be performed on a numpy ndarray using \verb+numpy.linalg.cholesky+. The Cholesky factorization can express the matrix in terms of a lower triangular matrix and its conjugate transpose (denoted with the $^*$ operator), for which it is much simpler to compute the inverse of a matrix. Letting $\mathbf{L}$ be a lower diagonal matrix,

\begin{equation}
(\mathbf{K} + \lambda \mathbf{I}) = \mathbf{L} \mathbf{L}^*
\end{equation}
\begin{equation}
(\mathbf{K} + \lambda \mathbf{I})^{-1} = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1}
\end{equation}
\begin{equation}
\boldsymbol\alpha = (\mathbf{L}^{-1})^* \;\mathbf{L}^{-1} \mathbf{y}
\end{equation}

Thus, an expression for $\boldsymbol\alpha$ is obtained in terms of the desired output $\mathbf{y}$ and the inverse of the Cholesky decomposition of the matrix $\mathbf{K} + \lambda \mathbf{I})$, bypassing the need to perform a direct inversion, which could be more computationally expensive.

To be able to perform the search repeatedly in a python script, an RBF object with a method for performing validation were created as shown below:

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{rbf_def}
\caption{the definition of the RBF class used for validation and testing}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{rbf_helper}
\caption{RBF class methods to perform subtasks}
\end{figure}

Using the RBF class and the \verb+run_validation+ class method, the RMSE loss across the dataset was found for various values of $\theta$ and $\lambda$ and the results written to a text file. The code for running validation and the output for both \verb+mauna_loa+ and \verb+rosenbrock+ are shown below.

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{validation}
\caption{the code used to evaluate RMSE loss across the grid of hyperparameters}
\end{figure}

% figure out how to make side-by-side subfigures
\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{mauna_loa_val}
\caption{results of validation across the hyperparamter grid for mauna\_loa}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{rosenbrock_val}
\caption{results of validation across the hyperparamter grid for rosenbrock}
\end{figure}

For both datasets, it was evident that increased values of $\lambda$ resulted in greater RMSE values when evaluating for the validation set, and this pattern was consistent even when testing for values of $\lambda$ lower than $0.001$. This finding suggests that our model is not prone to overfitting to the data, and that the introduction of a regularization term may actually be detrimental to the overall performance of the model.

\pagebreak
\fi
\section*{Question 4}
For the fourth question, we were tasked with implementing a greedy algorithm using a set of basis functions of our own design. In order to make justifiable decisions regarding the selection of basis functions, the \verb+mauna_loa+ dataset was plotted in order to make observations about the dataset (plot provided below). In order to form a set of basis functions, it was assumed that there was some underlying function for the dataset, and that each data point was sampled from this underlying function (with the addition of some error). With this assumption, by observing the data, one could presumably construct a numeric function from a set of basis functions that closely resembles the underlying function, rendering our model effective in interpolation and extrapolation.

% provide figure here!!!!

From the plot, a number of observations/claims could be made about the underlying pattern that produced the datapoints as they were observed. Firstly, the evidence shows strongly that there is some sinusoidal/periodic component to the underlying function. Additionally, the average value of the function appears to increase periods. Moreover, the average value across a period seems to increase monotonically at a rate that is somewhat faster than linear (convex). Finally, the magnitude of the oscillations appears to be relatively steady, suggesting that the periodic term and the monotonically increasing terms could be added as opposed to multiplied in the underlying function for the \verb+mauna_loa+ dataset.

From these observations, it made sense to construct a dictionary of basis functions from a set of function classes, and include in the dictionary several permutations from each function class. Each function class could be expressed mathematically as a function of the 'input` of the dataset (which in the context of \verb+mauna_loa+ is a date) and a few parameters that would be set differently for each permutation of the function in the dictionary. Firstly, to capture the periodic component of the underlying function, it was decided that the first class of functions used would be sinusoidal functions, with permutations varying in amplitude ($A$), frequency ($\omega$), and phase offset ($\phi$). Then, to capture the growth in average value across periods, a few different classes were added: a linear class of functions taking a slope parameter ($m$), an exponential class of functions taking offset, vertical scaling factor and horizontal compression factor.
Finally, rather than including parameters for vertical offset in each function classes, since the vertical offsets in each basis function would add anyways, they were omitted from individual classes and rather the final function class used was the class of linear functions with zero slope, that took a single vertical offset parameter ($b$). The four classes are denoted with $f_i(x)$ and are expressed in terms of their parameters below.

\begin{equation}
f_1(x) = A \sin(\omega x - \phi)
\end{equation}

\end{document}